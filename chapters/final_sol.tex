% !TEX root = ../tssp_exams.tex

\newpage
\thispagestyle{empty}
\section{Final exam solutions}


\subsection[2023-2024]{\hyperref[sec:kr_04_2023_2024]{2023-2024}}
\label{sec:sol_kr_04_2023_2024} % \label{ссылка сюда}



\begin{enumerate}

\item Let's observe that we may decompose $X_i$ as a sum $X_i = Y_i + \theta$,
where $Y_i \sim \dExp(\lambda)$. 

Hence, $\E(X_i) = 1/\lambda + \theta$, $\Var(X_i) = \Var(Y_i) = 1/\lambda^2$
and $\E(X_i^2) = 1/\lambda^2 + (1/\lambda + \theta)^2$.

There is an alternative solution with direct integration:
\[
\E(X_i) = \int_{\theta}^{+\infty} x f(x) \; dx, \quad \E(X_i^2) = \int_{\theta}^{+\infty} x^2 f(x) \; dx.
\]

\begin{enumerate}
    \item Solving $1/\hat\lambda + 1 = \bar X$ we obtain $\hat\lambda = 1/ (\bar X - 1)$.
    \item Solving for $\hat\lambda$ and $\hat\theta$ the system
\[
\begin{cases}
    1/\hat\lambda + \hat \theta = \bar X \\
    1/\hat\lambda^2 + (1/\hat\lambda + \hat\theta)^2 = M_2 \text{ with } M_2 = \sum X_i^2/n
\end{cases}    
\]
we obtain 
\[
\hat \lambda = \frac{1}{\sqrt{M_2 - \bar X^2}} , \quad \hat \theta =  \bar X - \sqrt{M_2 - \bar X^2}   
\]
\end{enumerate}

\item The log-likelihood function is equal to
\[
\ell(a) = \sum_{i=1}^n \left( (-0.5)\ln(4\pi) - 0.5 \ln a - (x_i - a)^2 / 4a \right).
\]
The equation $\ell'(a) = 0$ may be simplified to
\[
    n\hat a^2 + 2n\hat a - \sum X_i^2  = 0    
\]
Hence, 
\[
\hat a = \frac{-2n \pm \sqrt{4 n^2 + 4n \sum X_i^2}}{2n}
\]
We choose the root $\hat a > 0$ as $\Var(X_i) = 2a > 0$.
\[
\hat a = \sqrt{ 1  + \sum X_i^2/n} - 1
\]

Just for fun. 
In the case $X_i \sim \cN(a, ka)$ the equation would be
\[
    n\hat a^2 + k n\hat a - \sum X_i^2  = 0    
\]
And 
\[
\hat a = \frac{-nk + \sqrt{k^2 n^2  + 4n \sum X_i^2}}{2n}.    
\]



\item $\E(Y_i) = \P(X_i > 1) = (a-1) / a = p$.
\begin{enumerate}
    \item The estimator is consistent as
\[
\plim \hat a = \frac {1}{1- \plim \bar Y } = \frac{1}{1 - \frac{a-1}{a}} = a
\]
\item 
For $n=2$ we have the positive probability $p^2$ that $\bar Y = 1$.
Hence with positive probability $\hat a$ is not defined.
The value $\E(\hat a)$ does not exist for $n=2$.
\end{enumerate}

\item 
\begin{enumerate}
\item The log-likelihood function is equal to
\[
\ell(\lambda) = \sum_{i=1}^n \left( -\lambda + X_i \ln \lambda - \ln (X_i!)\right)    
\]
The score function is 
\[
\score(\lambda) = \ell'(\lambda) = \sum_{i=1}^n \left( -1 + X_i / \lambda\right).
\]
And
\[
\ell''(\lambda) = \sum_{i=1}^n \left(- X_i / \lambda^2 \right).
\]
Fisher information is 
\[
I_F = -\E(\ell''(\lambda)) = \sum \E(X_i) / \lambda^2 = n\lambda / \lambda^2 = n/\lambda.    
\]

\item Solving $\ell' = 0$ we obtain 
\[
\hat \lambda = \bar X    
\] 

\item Rewrite $\ell'(\lambda)$ using $\hat\lambda$. 
Be careful! Do not confound $\lambda$ and $\hat\lambda$. 
\[
\score(\lambda) = \ell'(\lambda) = -n + n \hat \lambda / \lambda.
\]
Hence the score function is linear function of $\hat\lambda$, $\Corr(\score(\lambda), \hat\lambda) = 1$
and the Cramer-Rao bound is attained. 

One may also  find $\E \hat \lambda = \lambda$, $\Var(\hat \lambda) = \lambda / n$ and
explicitly check that the general bound
\[
\Var(\hat \lambda) \geq 1/ I_F 
\]
is attained as equality in our case
\[
    \lambda / n = 1/(n/\lambda). 
\]


\end{enumerate}

\item We do not need the formula for $\Gamma(\alpha)$ here. 
\begin{enumerate}
    \item For known $\lambda = 1$ the likelihood is 
    \[
    L = \left(\prod X_i \right)^{\alpha - 1} \frac{1}{\Gamma(\alpha)} \cdot \exp( - \sum X_i).
    \]
    If we optimize this function for $\alpha$ the optimal $\hat\alpha$ will depend only on $\prod X_i$.
    Hence $\prod X_i$ is a sufficient statistic for $\alpha$. 
    There are many other sufficient statistics, $\sum \ln X_i$ is another example. 
    \item Now the likelihood is 
    \[
    L = \left(\prod X_i \right)^{\alpha - 1} \frac{1}{\Gamma(\alpha)} \lambda^{\alpha }\exp( - \lambda \sum X_i).
    \]
    If we optimize this function for $\alpha$ and $\lambda$ the optimal point will depend only on $\prod X_i$ and $\sum X_i$.
    Hence $\begin{pmatrix}
        \prod X_i & \sum X_i 
    \end{pmatrix}$ is a two dimensional sufficient statistic for $(\alpha, \lambda)$.
    
\end{enumerate}

\item 
\begin{enumerate}
    \item Under $H_0$ we have $X_i \sim \dPois(\lambda)$, $Y_i \sim \dPois(\lambda)$.
    \[
        \ell(\lambda) = \sum_{i=1}^{n_x} \left( -\lambda + X_i \ln \lambda - \ln (X_i!)\right) + 
        \sum_{i=1}^{n_y} \left( -\lambda + Y_i \ln \lambda - \ln (Y_i!)\right)       
    \]
    The score function is 
    \[
    \score(\lambda) = \ell'(\lambda) = \sum_{i=1}^{n_x} \left( -1 + X_i / \lambda\right) + \sum_{i=1}^{n_y} \left( -1 + Y_i / \lambda\right).
    \]
    The estimator is $\hat \lambda = (\sum X_i + \sum Y_i) / (n_x + n_y)$.
    \[
    \max \ell_R = - \hat \lambda (n_x + n_y) + \left(\sum X_i + \sum Y_i \right)\ln \hat\lambda - \sum \ln X_i! - \sum \ln Y_i!    
    \]
    
    \item In unrestricted model we have two independent estimators, 
    \[
    \hat \lambda_x = \bar X, \quad \hat \lambda_y = \bar Y    
    \]
    \[
    \max \ell_{UR} = - \hat \lambda_x n_x + \sum X_i \ln \hat\lambda_x + \sum Y_i \ln \hat\lambda_y - \sum \ln X_i! - \sum \ln Y_i!    
    \]

    \item 
    \[
    LR = 2(\max \ell_{UR} - \max \ell_R) = 2 \sum X_i (\ln \hat \lambda_x - \ln \hat\lambda) + 2 \sum Y_i (\ln \hat \lambda_y - \ln \hat\lambda)
    \]
    \item Unrestricted model has two parameters, restricted model has one parameter, 
    hence we use chi-squared disribution with $2 - 1 = 1$ degree of freedom, $LR_{\crit} = 3.84$.
    We calculate estimates, $\hat \lambda_x = 5$, $\hat\lambda_y = 4.5$, $\hat\lambda = 14/3$.
    
    \[
    LR = 1000 (\ln 5 - \ln (14/3)) + 1800 (\ln 4.5 - \ln (14/3)) \approx 3.5
    \]
    We do not reject $H_0$.
    


\end{enumerate}

\end{enumerate}



\subsection[2022-2023]{\hyperref[sec:kr_04_2022_2023]{2022-2023}}
\label{sec:sol_kr_04_2022_2023} % \label{ссылка сюда}



\begin{enumerate}

\item 


\end{enumerate}
    


\subsection[2021-2022]{\hyperref[sec:kr_04_2021_2022]{2021-2022}}
\label{sec:sol_kr_04_2021_2022} % \label{ссылка сюда}



\begin{enumerate}

\item 


\end{enumerate}
    

% \subsection[что идет в оглавление]{\hyperref[на что ссылка]{текст ссылки}}
\subsection[2020-2021]{\hyperref[sec:kr_04_2020_2021]{2020-2021}}
\label{sec:sol_kr_04_2020_2021} % \label{ссылка сюда}


\begin{enumerate}
\item Let's draw the chain

\begin{center}
\begin{tikzpicture}[->, >=stealth', auto, semithick, node distance=3cm]
\tikzstyle{every state}=[fill=white,draw=black,thick,text=black,scale=1]
\node[state]    (A)                     {$S$};
\node[state]    (B)[right of=A]   {$6$};
\node[state]    (C)[right of=B]   {$62$};
\node[state]    (D)[right of=C]   {$626$};
\path
(A) edge[loop below]     node{}         (A)
    edge                node{}     (B)
(B) edge                node{}           (C)
    edge[loop below]    node{}           (B)
    edge[bend right]    node{}           (A)
(C) edge                node{}           (D)
    edge[bend right]                node{}           (B)
    edge[bend right=40]         node{}           (A);
\end{tikzpicture}
\end{center}

The system of equations for expected values:
\[
\begin{cases}
x_s = 1 + \frac{1}{6} x_6 + \frac{5}{6} x_s \\
x_6 = 1 + \frac{1}{6} x_6 + \frac{1}{6} x_{62}  + \frac{4}{6} x_s \\
x_{62} = 1 + \frac{1}{6} \cdot 0 + \frac{1}{6} x_{6}  + \frac{4}{6} x_s \\
\end{cases}    
\]


The system of equations for moment generating functions:
\[
\begin{cases}
m_s(t) = \exp(t) \left(\frac{1}{6} m_6(t) + \frac{5}{6} m_s(t)\right) \\
m_6(t) = \exp(t) \left( \frac{1}{6} m_6(t) + \frac{1}{6} m_{62}(t)  + \frac{4}{6} m_s(t)\right) \\
m_{62}(t) = \exp(t) \left( \frac{1}{6} \cdot 1 + \frac{1}{6} m_{6}(t)  + \frac{4}{6} m_s(t) \right)\\
\end{cases}    
\]


\item \begin{enumerate}
    \item Let's denote by $x$ all available information, 
    \[
    x = \begin{pmatrix}
        y_{100} \\
        y_{99} \\
        y_{98} \\
        u_{99}
    \end{pmatrix}    
    \]
    Let's use $t=100$:
    \[
    y_{100} = 1 + 0.5 y_{98} + u_{100} + u_{99}    
    \]

    Using all available information we obtain $u_{100}  = 1.5$ and hence
    \[
    y_{101} \mid x \sim  \cN(1 + 0.5 y_{99} + u_{100} ; 4)
    \]

    \item Here we work with true betas:
    \[
    \E(y_{101} \mid y_{100}) = \mu_y + \frac{\Cov(y_{100}, y_{101})}{\Var(y_{100})}(y_{100} - \mu_y)    
    \]

\end{enumerate}
\item \begin{enumerate}
    \item Moment generating function
\[
m_N(t) = \sum_{j=0} \exp(tj) (1-h)^j h = h \sum_{j=0} (\exp(t) (1-h))^j = \frac{h}{1 - \exp(t) (1 - h)}  
\]
\item As $S = N_1 + N_2 + \ldots + N_k$:
\[
m_S(t) =  \left( \frac{h}{1 - \exp(t) (1 - h)} \right)^k
\]
\item Due to my mistake the limit is easy, $0$. 

In my dream it was $k\to \infty$, $k \cdot (1 - h) \to 0.5$ and that would be fun!

\end{enumerate}

\item \begin{enumerate}
    \item Let's use Ito's lemma
    \[
    dX_t = f'(t) \cos (2021 W_t) dt - 2021 f(t) \sin (2021 W_t) dW_t + \frac{1}{2}2021^2 f(t) \cos(2021 W_t) dt    
    \]
    \item To make $X_t$ a martingale we should kill $dt$ term. 
    \item As $X_t$ is martingale $\E(X_t) = \E(X_0) = f(0)$.
    So $\E(\cos (2021 W_t)) = f(0) / f(t)$.
\end{enumerate}

\end{enumerate}



